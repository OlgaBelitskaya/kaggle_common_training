# -*- coding: utf-8 -*-
"""digit-recognition-models-3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y6BLgtNXL1jONiZ5JYnTfBuixC0tWijb

<h1 class='font-effect-3d' style='font-family:Monoton; color:#ff00cc;'>Python Modules, Styling, and Helpful Functions</h1>
"""

# Commented out IPython magic to ensure Python compatibility.
# %%html
# <style> 
# @import url('https://fonts.googleapis.com/css?family=Monoton|Roboto&effect=3d'); 
# h1 {text-shadow:4px 4px 4px #aaa;} 
# span {font-family:Roboto; color:midnightblue; text-shadow:4px 4px 4px #aaa;}
# div.output_prompt {color:#ff6eff;}
# div.input_prompt {color:#ff00cc;} 
# div.output_area pre,div.output_subarea {font-family:Roboto; font-size:15px; color:#ff6eff;}
# div.output_stderr pre {background-color:ghostwhite;}
# </style>

import warnings; warnings.filterwarnings('ignore')
import pandas as pd,numpy as np,xgboost as xgb
import pylab as pl,seaborn as sn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn import metrics,neighbors,ensemble
N1,N2=10**4,10**3

def ohe(x): 
    return OneHotEncoder(n_values='auto').fit(x.reshape(-1,1))\
           .transform(x.reshape(-1,1)).toarray().astype('int64')
def tts(X,y): 
    x_train,x_test,y_train,y_test=\
    train_test_split(X,y,test_size=.2,random_state=1)
    n=int(len(x_test)/2)
    x_valid,y_valid=x_test[:n],y_test[:n]
    x_test,y_test=x_test[n:],y_test[n:]
    return x_train,x_valid,x_test,y_train,y_valid,y_test
def deprocess(x):
    x=(x/2+.5)*255
    x=np.clip(x,0,255)
    x=np.uint8(x)
    return x.reshape(28,28)
def display_images(images):
    n_images=len(images)
    rows=4; cols=n_images//rows    
    pl.figure(figsize=(cols,rows))    
    for i in range(n_images):
        img=deprocess(images[i])
        pl.subplot(rows,cols,i+1)
        pl.imshow(img,cmap=pl.cm.bone)
        pl.xticks([]); pl.yticks([])
    pl.tight_layout(); pl.show()

"""<h1 class='font-effect-3d' style='font-family:Monoton; color:#ff00cc;'> Data Loading and Preprocessing</h1>"""

train=pd.read_csv("../input/train.csv")
train_labels=train["label"].values.astype('int16')
ctrain_labels=ohe(train_labels)
train_images=train.drop(labels=["label"],axis=1) 
train_images=train_images.values.astype('float32')/255
train_images=(train_images-.5)*2
train_labels.shape,ctrain_labels.shape,train_images.shape

pl.figure(figsize=(12,3))
sn.countplot(train_labels,palette="cool")
pl.title('Distribution of Train Labels');

x_train,x_valid,x_test,\
y_train,y_valid,y_test=tts(train_images,ctrain_labels)
ny_train=np.array([np.argmax(y) for y in y_train])
ny_valid=np.array([np.argmax(y) for y in y_valid])
ny_test=np.array([np.argmax(y) for y in y_test])
x_train2,x_valid2,x_test2=x_train[:N1],x_valid[:N2],x_test[:N2]
y_train2,y_valid2,y_test2=y_train[:N1],y_valid[:N2],y_test[:N2]
ny_train2,ny_valid2,ny_test2=ny_train[:N1],ny_valid[:N2],ny_test[:N2]
sh=[el.shape for el in [x_train,x_valid,x_test,
                        y_train,y_valid,y_test]]
pd.DataFrame(sh)

display_images(x_train[:32])

"""<h1 class='font-effect-3d' style='font-family:Monoton; color:#ff00cc;'>Neighbors & Ensemble Classifiers</h1>"""

classifier_list,classifier_names,clf_datasets=[],[],[]
acc_train,acc_test,loss_train,loss_test=[],[],[],[]

def classifier_fit_score(classifier,classifier_name,clf_dataset, 
                         x_train,x_test,y_train,y_test):    
    classifier_list.append(str(classifier))
    classifier_names.append(str(classifier_name))
    clf_datasets.append(str(clf_dataset))    
    clf=classifier; clf.fit(x_train,y_train)    
    y_clf_train=clf.predict(x_train)
    y_clf_test=clf.predict(x_test)        
    acc_clf_train=round(metrics.accuracy_score(y_train,y_clf_train),6)
    acc_train.append(acc_clf_train)
    acc_clf_test=round(metrics.accuracy_score(y_test,y_clf_test),6)
    acc_test.append(acc_clf_test)   
    loss_clf_train=round(metrics.hamming_loss(y_train,y_clf_train),4)
    loss_train.append(loss_clf_train)
    loss_clf_test = round(metrics.hamming_loss(y_test,y_clf_test),4)
    loss_test.append(loss_clf_test)    
    return [y_clf_train,y_clf_test,acc_clf_train,acc_clf_test, 
            loss_clf_train,loss_clf_test]

def get_classifier_results():
    results={'classifier':classifier_list,'classifier_name':classifier_names,
            'clf_dataset':clf_datasets,'acc_train':acc_train,'acc_test':acc_test,
            'loss_train':loss_train,'loss_test':loss_test}
    df_results=pd.DataFrame(results)   
    df_list=['classifier_name','acc_train','acc_test','loss_train','loss_test']               
    return df_results,df_results[df_list]

clf1=neighbors.KNeighborsClassifier()
clf2=ensemble.RandomForestClassifier(n_estimators=64,max_depth=11) 
clf3=ensemble.GradientBoostingClassifier()

y_knc_train,y_knc_test=\
classifier_fit_score(clf1,'KNeighborsClassifier','digits',
                     x_train2,x_test2,ny_train2,ny_test2)[:2]

y_rfc_train,y_rfc_test=\
classifier_fit_score(clf2,'RandomForestClassifier','digits',
                     x_train2,x_test2,ny_train2,ny_test2)[:2]

y_gbc_train,y_gbc_test=\
classifier_fit_score(clf3,'GradientBoostingClassifier','digits',
                     x_train2,x_test2,ny_train2,ny_test2)[:2]

"""<h1 class='font-effect-3d' style='font-family:Monoton; color:#ff00cc;'>Classification  Results</h1>"""

df_results,df_results2=get_classifier_results()
df_results2.sort_values('acc_test',ascending=False)

pl.figure(figsize=(12,8)); n=100; x=range(n)
pl.scatter(x,ny_test2[:n],marker='*',s=240, 
            color='slategray',alpha=.7,label='Real data')
pl.scatter(x,y_knc_test[:n],marker='s',s=60, 
            color='purple',label='K Neighbors Classifier')
pl.scatter(x,y_rfc_test[:n],marker='v',s=40, 
color='darkblue',label='Random Forest Classifier')
pl.scatter(x,y_gbc_test[:n],marker='o',s=20, 
color='magenta',label='Gradient Boosting Classifier')
pl.xlabel('Observations',fontsize=12)
pl.ylabel('Targets',fontsize=12)
pl.title('Classifiers. Test Data',fontsize=15)
pl.legend(bbox_to_anchor=[1.,.6],fontsize=12);

"""<h1 class='font-effect-3d' style='font-family:Monoton; color:#ff00cc;'>XGB Classifier</h1>"""

clf=xgb.XGBClassifier(objective="multi:softprob",verbosity=2,
                      random_state=42,max_depth=11)
clf.fit(x_train2,ny_train2,eval_metric='mlogloss',
        eval_set=[(x_train2,ny_train2),(x_valid2,ny_valid2)])
print(clf.evals_result())

y_xgb_train=clf.predict(x_train2)
y_xgb_valid=clf.predict(x_valid2)
y_xgb_test=clf.predict(x_test2)
for [y,py] in [[ny_train2,y_xgb_train],
               [ny_valid2,y_xgb_valid],
               [ny_test2,y_xgb_test]]:
    print(metrics.accuracy_score(y,py))
    print(metrics.confusion_matrix(y,py))